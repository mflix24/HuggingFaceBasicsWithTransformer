{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Basics With Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing libraries through pip\n",
    "    # ! pip install transformers\n",
    "    # ! pip install datasets\n",
    "    # ! pip install tokenizer\n",
    "    # ! pip install seqeval\n",
    "\n",
    "# you can also install all of the library through the command line below:\n",
    "    # !pip install transformers datasets tokenizer seqeval\n",
    "\n",
    "# warnings removed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# warnings removed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "# Loading conll2003 dataset fails because it was removed on 1/14/2022) \n",
    "# from the location it is looking for. colab solves this problem\n",
    "# but I need to upload the file thus I am using vs code.\n",
    "conll2003=datasets.load_dataset('conll2003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the different dataset of train and test and validation for the first index\n",
    "print(conll2003['train'][0])\n",
    "print(conll2003['validation'][0])\n",
    "print(conll2003['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the whole dataset details information\n",
    "print(conll2003['train'])\n",
    "print(conll2003['test'])\n",
    "print(conll2003['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QQQ : what is pos_tags, chunk_tags and ner_tags with respect to NLP dataset\n",
    "\n",
    "\n",
    "AAA :\n",
    "In the context of natural language processing (NLP) datasets, POS tags, chunk tags, and NER tags are all labels assigned to individual words (or tokens) in a sentence. These labels provide different levels of information about the words and how they function within the sentence.\n",
    "\n",
    "Here's a breakdown of each:\n",
    "\n",
    "POS tags (Part-of-Speech tags): These tags identify the grammatical function of a word in a sentence. Common POS tags include nouns (NN), verbs (VB), adjectives (JJ), adverbs (RB), etc. These tags help understand the basic building blocks of a sentence.\n",
    "\n",
    "Chunk tags: These tags group words together based on their syntactic role within a phrase. They identify chunks like noun phrases (NP), verb phrases (VP), prepositional phrases (PP), etc. Chunk tags help understand how words relate to each other within smaller units of meaning.\n",
    "\n",
    "NER tags (Named Entity Recognition tags): These tags identify and classify named entities within a sentence. Entities can be people (PER), locations (LOC), organizations (ORG), dates (DATE), money (MONEY), etc. NER tags help extract specific information from text.\n",
    "\n",
    "Here's an analogy:\n",
    "\n",
    "Imagine a sentence as a building. POS tags identify the individual building materials (bricks, wood, etc.). Chunk tags group these materials into walls, roofs, and floors. NER tags identify specific features within the building, like doors or windows.\n",
    "\n",
    "How they appear in datasets:\n",
    "\n",
    "These tags are often included in NLP datasets alongside the actual words in a sentence. Each word might have a corresponding POS tag, chunk tag, and NER tag associated with it. The specific format for these tags can vary depending on the dataset, but common formats include:\n",
    "\n",
    "IOB or IOB2: These formats use prefixes like B- (beginning), I- (inside), and O- (outside) to indicate the position of a word within a chunk or named entity.\n",
    "Why they are important:\n",
    "\n",
    "These tags are crucial for various NLP tasks. They are used to train machine learning models for tasks like:\n",
    "\n",
    "Part-of-speech tagging: Automatically assigning POS tags to new sentences.\n",
    "Syntactic parsing: Understanding the sentence structure and relationships between words.\n",
    "Named entity recognition: Extracting specific types of information from text.\n",
    "Machine translation: Understanding the grammatical structure of both source and target languages.\n",
    "By understanding the meaning of these tags, you can better interpret NLP datasets and leverage them for various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing value into a varibale as example_text\n",
    "example_text = conll2003['train'][0]\n",
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting words into ids\n",
    "tokenized_input = tokenizer(example_text[\"tokens\"], is_split_into_words=True)\n",
    "# tokenized_input = tokenizer(example_text[\"tokens\"])\n",
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now converting input_ids into tokens. vice versa for previous cell.\n",
    "# here tokens means word. each word means each tokens.\n",
    "# here few things should be noted like wise the function returns a list type data\n",
    "# the list stats with another list CLS means classification and another list is used here as SEP menas Seperator.\n",
    "# Each sentence has these list like CLS and SEP. It means sentences are seperated by these two created list like CLS and SEP\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
