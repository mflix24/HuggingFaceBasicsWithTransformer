This repository is created for understanding the concept of Transformers
and how we can use HuggingFace with these tranformers. Tranformer has only
one inner concept which is Self Attention. Tranformer always follow the session,
it never forgets togenerate what we need. Transformer has basically two part, one is encode and another is decoder. The giant tech companies are using differently like few are using encoder part and few are using decoder part. The most elits companies are using both. Every aspect has cons and pros. We have to consider it.